# -*- coding: utf-8 -*-
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä SEO –∫–∞—á–µ—Å—Ç–≤–∞ —Å—Ç–∞—Ç–µ–π
"""
from telebot import types
from loader import bot
import requests
from bs4 import BeautifulSoup
import re


def analyze_seo_content(html_content, url):
    """
    –ê–Ω–∞–ª–∏–∑ SEO –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç–∞—Ç—å–∏
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ü–µ–Ω–∫—É 0-100 –∏ –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    
    score = 100
    issues = []
    warnings = []
    positives = []
    
    # ==========================================
    # 1. –ó–ê–ì–û–õ–û–í–ö–ò (H1, H2, H3)
    # ==========================================
    h1_tags = soup.find_all('h1')
    h2_tags = soup.find_all('h2')
    h3_tags = soup.find_all('h3')
    
    # H1 - –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–¥–∏–Ω
    if len(h1_tags) == 0:
        issues.append("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ H1")
        score -= 15
    elif len(h1_tags) > 1:
        issues.append(f"‚ùå –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ H1 ({len(h1_tags)} —à—Ç). –î–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–¥–∏–Ω")
        score -= 10
    else:
        h1_text = h1_tags[0].get_text().strip()
        if len(h1_text) < 20:
            warnings.append(f"‚ö†Ô∏è H1 —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π ({len(h1_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            score -= 5
        elif len(h1_text) > 70:
            warnings.append(f"‚ö†Ô∏è H1 —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π ({len(h1_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            score -= 3
        else:
            positives.append(f"‚úÖ H1 –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã ({len(h1_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ—Å–ø–∞–º –∫–ª—é—á–µ–π –≤ H1
        words = h1_text.lower().split()
        if len(words) != len(set(words)):
            issues.append("‚ùå –í H1 –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è —Å–ª–æ–≤–∞ (keyword stuffing)")
            score -= 10
    
    # H2 - –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 6-10 —à—Ç—É–∫
    if len(h2_tags) < 4:
        warnings.append(f"‚ö†Ô∏è –ú–∞–ª–æ –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ H2 ({len(h2_tags)} —à—Ç). –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 6-10")
        score -= 5
    elif len(h2_tags) > 12:
        warnings.append(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ H2 ({len(h2_tags)} —à—Ç)")
        score -= 3
    else:
        positives.append(f"‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ H2 ({len(h2_tags)} —à—Ç)")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ emoji –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # —ç–º–æ–¥–∑–∏ –ª–∏—Ü
        u"\U0001F300-\U0001F5FF"  # —Å–∏–º–≤–æ–ª—ã
        u"\U0001F680-\U0001F6FF"  # —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç
        u"\U0001F1E0-\U0001F1FF"  # —Ñ–ª–∞–≥–∏
        u"\U00002700-\U000027BF"  # –¥–∏–Ω–≥–±–∞—Ç—ã
        "]+", flags=re.UNICODE)
    
    emoji_in_h2 = sum(1 for h2 in h2_tags if emoji_pattern.search(h2.get_text()))
    if emoji_in_h2 > 2:
        warnings.append(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ emoji –≤ H2 ({emoji_in_h2} —à—Ç)")
        score -= 5
    
    # ==========================================
    # 2. –ö–û–ù–¢–ï–ù–¢ –ò –¢–ï–ö–°–¢
    # ==========================================
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
    text = soup.get_text(separator=' ', strip=True)
    words = text.split()
    word_count = len(words)
    
    if word_count < 800:
        issues.append(f"‚ùå –°–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ ({word_count} —Å–ª–æ–≤). –ú–∏–Ω–∏–º—É–º 1000")
        score -= 10
    elif word_count < 1200:
        warnings.append(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞ ({word_count} —Å–ª–æ–≤). –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 1500+")
        score -= 5
    elif word_count > 5000:
        warnings.append(f"‚ö†Ô∏è –û—á–µ–Ω—å –º–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ ({word_count} —Å–ª–æ–≤)")
        score -= 3
    else:
        positives.append(f"‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –æ–±—ä–µ–º ({word_count} —Å–ª–æ–≤)")
    
    # ==========================================
    # 3. –ü–õ–û–¢–ù–û–°–¢–¨ –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í
    # ==========================================
    
    # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏–∑ H1
    if h1_tags:
        h1_words = h1_tags[0].get_text().lower().split()
        # –ë–µ—Ä–µ–º 2-3 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤–∞ (–∏—Å–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥–ª–æ–≥–∏)
        stop_words = {'–¥–ª—è', '–∫–∞–∫', '—á—Ç–æ', '—ç—Ç–æ', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–æ', '–æ–±', '–∏', '–∏–ª–∏'}
        keywords = [w for w in h1_words if w not in stop_words and len(w) > 3][:3]
        
        if keywords:
            # –°—á–∏—Ç–∞–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
            text_lower = text.lower()
            for keyword in keywords:
                count = text_lower.count(keyword)
                density = (count / word_count) * 100 if word_count > 0 else 0
                
                if density > 2.5:
                    issues.append(f"‚ùå –ü–µ—Ä–µ—Å–ø–∞–º –∫–ª—é—á–∞ '{keyword}': {count} —Ä–∞–∑ ({density:.1f}%)")
                    score -= 15
                elif density > 1.5:
                    warnings.append(f"‚ö†Ô∏è –í—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å '{keyword}': {count} —Ä–∞–∑ ({density:.1f}%)")
                    score -= 5
                elif density < 0.5 and count < 3:
                    warnings.append(f"‚ö†Ô∏è –ú–∞–ª–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π '{keyword}': –≤—Å–µ–≥–æ {count} —Ä–∞–∑")
                    score -= 3
                else:
                    positives.append(f"‚úÖ –ü–ª–æ—Ç–Ω–æ—Å—Ç—å '{keyword}' –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞: {count} —Ä–∞–∑ ({density:.1f}%)")
    
    # ==========================================
    # 4. AI-–ö–õ–ò–®–ï –ò –®–ê–ë–õ–û–ù–´
    # ==========================================
    
    ai_phrases = [
        '–¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä—ë–º',
        '–¥–∞–≤–∞–π—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º',
        '–≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –º–∏—Ä–µ',
        '–∫–∞–∫ –∏–∑–≤–µ—Å—Ç–Ω–æ',
        '–Ω–∞ —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–π –¥–µ–Ω—å',
        '–Ω–∞—Å—Ç–æ—è—â–∏–º —Å–ø–∞—Å–µ–Ω–∏–µ–º',
        '–∏–¥–µ–∞–ª—å–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º',
        '—Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å',
        '–≤–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å',
        '–≤ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ —Ö–æ—á–µ—Ç—Å—è —Å–∫–∞–∑–∞—Ç—å'
    ]
    
    text_lower = text.lower()
    found_phrases = [phrase for phrase in ai_phrases if phrase in text_lower]
    
    if len(found_phrases) > 5:
        issues.append(f"‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã AI-–∫–ª–∏—à–µ ({len(found_phrases)} —à—Ç): {', '.join(found_phrases[:3])}...")
        score -= 15
    elif len(found_phrases) > 2:
        warnings.append(f"‚ö†Ô∏è AI-–∫–ª–∏—à–µ ({len(found_phrases)} —à—Ç): {', '.join(found_phrases)}")
        score -= 8
    elif len(found_phrases) > 0:
        warnings.append(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ –∫–ª–∏—à–µ: {', '.join(found_phrases)}")
        score -= 3
    else:
        positives.append("‚úÖ AI-–∫–ª–∏—à–µ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")
    
    # ==========================================
    # 5. –°–¢–†–£–ö–¢–£–†–ê –ò –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–ï
    # ==========================================
    
    # –°–ø–∏—Å–∫–∏
    ul_tags = soup.find_all('ul')
    ol_tags = soup.find_all('ol')
    list_count = len(ul_tags) + len(ol_tags)
    
    if list_count < 2:
        warnings.append(f"‚ö†Ô∏è –ú–∞–ª–æ —Å–ø–∏—Å–∫–æ–≤ ({list_count} —à—Ç)")
        score -= 5
    else:
        positives.append(f"‚úÖ –ï—Å—Ç—å —Å–ø–∏—Å–∫–∏ ({list_count} —à—Ç)")
    
    # –¢–∞–±–ª–∏—Ü—ã
    tables = soup.find_all('table')
    if len(tables) > 0:
        positives.append(f"‚úÖ –ï—Å—Ç—å —Ç–∞–±–ª–∏—Ü—ã ({len(tables)} —à—Ç)")
    else:
        warnings.append("‚ö†Ô∏è –ù–µ—Ç —Ç–∞–±–ª–∏—Ü (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è —Ü–µ–Ω/—Å—Ä–∞–≤–Ω–µ–Ω–∏–π)")
        score -= 3
    
    # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    images = soup.find_all('img')
    if len(images) < 2:
        warnings.append(f"‚ö†Ô∏è –ú–∞–ª–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ({len(images)} —à—Ç)")
        score -= 5
    else:
        positives.append(f"‚úÖ –ï—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è ({len(images)} —à—Ç)")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ ALT-—Ç–µ–∫—Å—Ç–∞
        images_without_alt = sum(1 for img in images if not img.get('alt'))
        if images_without_alt > 0:
            warnings.append(f"‚ö†Ô∏è –£ {images_without_alt} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ—Ç ALT-—Ç–µ–∫—Å—Ç–∞")
            score -= 3
    
    # ==========================================
    # 6. –í–ù–£–¢–†–ï–ù–ù–ò–ï –°–°–´–õ–ö–ò
    # ==========================================
    
    links = soup.find_all('a', href=True)
    internal_links = [link for link in links if link.get('href', '').startswith('/') or url.split('/')[2] in link.get('href', '')]
    
    if len(internal_links) < 2:
        warnings.append(f"‚ö†Ô∏è –ú–∞–ª–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ ({len(internal_links)} —à—Ç)")
        score -= 5
    elif len(internal_links) > 10:
        warnings.append(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Å—Å—ã–ª–æ–∫ ({len(internal_links)} —à—Ç)")
        score -= 3
    else:
        positives.append(f"‚úÖ –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∞ ({len(internal_links)} —Å—Å—ã–ª–æ–∫)")
    
    # ==========================================
    # 7. –°–•–ï–ú–ê –†–ê–ó–ú–ï–¢–ö–ò (Schema.org)
    # ==========================================
    
    schema_scripts = soup.find_all('script', type='application/ld+json')
    if len(schema_scripts) >= 2:
        positives.append(f"‚úÖ –ï—Å—Ç—å Schema.org —Ä–∞–∑–º–µ—Ç–∫–∞ ({len(schema_scripts)} —Å—Ö–µ–º)")
    elif len(schema_scripts) == 1:
        warnings.append("‚ö†Ô∏è –¢–æ–ª—å–∫–æ –æ–¥–Ω–∞ Schema.org —Å—Ö–µ–º–∞ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 2+)")
        score -= 3
    else:
        warnings.append("‚ö†Ô∏è –ù–µ—Ç Schema.org —Ä–∞–∑–º–µ—Ç–∫–∏")
        score -= 5
    
    # ==========================================
    # 8. –ß–ò–¢–ê–ë–ï–õ–¨–ù–û–°–¢–¨
    # ==========================================
    
    paragraphs = soup.find_all('p')
    if paragraphs:
        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –∞–±–∑–∞—Ü–∞
        avg_p_length = sum(len(p.get_text().split()) for p in paragraphs) / len(paragraphs)
        
        if avg_p_length > 100:
            warnings.append(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –∞–±–∑–∞—Ü—ã (–≤ —Å—Ä–µ–¥–Ω–µ–º {int(avg_p_length)} —Å–ª–æ–≤)")
            score -= 5
        elif avg_p_length < 20:
            warnings.append(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∞–±–∑–∞—Ü—ã (–≤ —Å—Ä–µ–¥–Ω–µ–º {int(avg_p_length)} —Å–ª–æ–≤)")
            score -= 3
        else:
            positives.append(f"‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∞–±–∑–∞—Ü–µ–≤ ({int(avg_p_length)} —Å–ª–æ–≤)")
    
    # ==========================================
    # 9. CTA (–ü—Ä–∏–∑—ã–≤—ã –∫ –¥–µ–π—Å—Ç–≤–∏—é)
    # ==========================================
    
    cta_keywords = ['–∑–≤–æ–Ω–∏—Ç–µ', '–∑–∞–∫–∞–∂–∏—Ç–µ', '—É–∑–Ω–∞–π—Ç–µ', '–æ—Å—Ç–∞–≤—å—Ç–µ –∑–∞—è–≤–∫—É', '–±–µ—Å–ø–ª–∞—Ç–Ω–æ', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è']
    cta_count = sum(text_lower.count(keyword) for keyword in cta_keywords)
    
    if cta_count < 2:
        warnings.append("‚ö†Ô∏è –ú–∞–ª–æ –ø—Ä–∏–∑—ã–≤–æ–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é")
        score -= 5
    else:
        positives.append(f"‚úÖ –ï—Å—Ç—å –ø—Ä–∏–∑—ã–≤—ã –∫ –¥–µ–π—Å—Ç–≤–∏—é ({cta_count} —à—Ç)")
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º score –¥–∏–∞–ø–∞–∑–æ–Ω–æ–º 0-100
    score = max(0, min(100, score))
    
    return {
        'score': score,
        'word_count': word_count,
        'h1_count': len(h1_tags),
        'h2_count': len(h2_tags),
        'images_count': len(images),
        'links_count': len(internal_links),
        'issues': issues,
        'warnings': warnings,
        'positives': positives
    }


@bot.callback_query_handler(func=lambda call: call.data.startswith("analyze_article_"))
def handle_analyze_article(call):
    """–ê–Ω–∞–ª–∏–∑ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""
    parts = call.data.split("_")
    category_id = int(parts[2])
    bot_id = int(parts[3])
    url = "_".join(parts[4:])  # URL –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å _
    
    user_id = call.from_user.id
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å
    bot.answer_callback_query(call.id, "üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å—Ç–∞—Ç—å—é...")
    
    try:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        analyzing_msg = bot.send_message(
            call.message.chat.id,
            "‚è≥ <b>–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å—Ç–∞—Ç—å—é...</b>\n\n"
            "–ü—Ä–æ–≤–µ—Ä—è—é:\n"
            "‚Ä¢ SEO –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é\n"
            "‚Ä¢ –°—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞\n"
            "‚Ä¢ –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤\n"
            "‚Ä¢ AI-–∫–ª–∏—à–µ\n"
            "‚Ä¢ –ß–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å",
            parse_mode='HTML'
        )
        
        # –°–∫–∞—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
        analysis = analyze_seo_content(response.text, url)
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        bot.delete_message(call.message.chat.id, analyzing_msg.message_id)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        score = analysis['score']
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–≤–µ—Ç –æ—Ü–µ–Ω–∫–∏
        if score >= 80:
            score_emoji = "üü¢"
            score_text = "–û—Ç–ª–∏—á–Ω–æ"
        elif score >= 60:
            score_emoji = "üü°"
            score_text = "–•–æ—Ä–æ—à–æ"
        elif score >= 40:
            score_emoji = "üü†"
            score_text = "–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ"
        else:
            score_emoji = "üî¥"
            score_text = "–¢—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏"
        
        text = (
            f"üìä <b>–≠–ö–°–ü–ï–†–¢–ù–´–ô –ê–ù–ê–õ–ò–ó –°–¢–ê–¢–¨–ò</b>\n\n"
            f"{score_emoji} <b>–û—Ü–µ–Ω–∫–∞: {score}/100</b> ‚Äî {score_text}\n\n"
            f"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            f"üìà <b>–°–¢–ê–¢–ò–°–¢–ò–ö–ê:</b>\n"
            f"‚Ä¢ –°–ª–æ–≤: {analysis['word_count']:,}\n"
            f"‚Ä¢ –ó–∞–≥–æ–ª–æ–≤–∫–æ–≤ H1: {analysis['h1_count']}\n"
            f"‚Ä¢ –ü–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ H2: {analysis['h2_count']}\n"
            f"‚Ä¢ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {analysis['images_count']}\n"
            f"‚Ä¢ –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫: {analysis['links_count']}\n\n"
        )
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
        if analysis['issues']:
            text += "üî¥ <b>–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–ë–õ–ï–ú–´:</b>\n"
            for issue in analysis['issues'][:5]:  # –ú–∞–∫—Å 5
                text += f"{issue}\n"
            text += "\n"
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        if analysis['warnings']:
            text += "üü° <b>–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø:</b>\n"
            for warning in analysis['warnings'][:5]:  # –ú–∞–∫—Å 5
                text += f"{warning}\n"
            text += "\n"
        
        # –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã
        if analysis['positives']:
            text += "üü¢ <b>–°–ò–õ–¨–ù–´–ï –°–¢–û–†–û–ù–´:</b>\n"
            for positive in analysis['positives'][:5]:  # –ú–∞–∫—Å 5
                text += f"{positive}\n"
            text += "\n"
        
        text += "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if score < 80:
            text += "üí° <b>–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:</b>\n"
            if score < 60:
                text += "‚Ä¢ –ò—Å–ø—Ä–∞–≤—å—Ç–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã\n"
                text += "‚Ä¢ –î–æ–±–∞–≤—å—Ç–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞\n"
                text += "‚Ä¢ –£–ª—É—á—à–∏—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç–∞—Ç—å–∏\n"
            else:
                text += "‚Ä¢ –£—Å—Ç—Ä–∞–Ω–∏—Ç–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è\n"
                text += "‚Ä¢ –î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫\n"
        else:
            text += "‚ú® <i>–°—Ç–∞—Ç—å—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º SEO!</i>"
        
        markup = types.InlineKeyboardMarkup()
        markup.row(
            types.InlineKeyboardButton("üåê –û—Ç–∫—Ä—ã—Ç—å —Å—Ç–∞—Ç—å—é", url=url)
        )
        markup.row(
            types.InlineKeyboardButton("üîô –ù–∞–∑–∞–¥", callback_data=f"platform_menu_manage_{category_id}_{bot_id}_website_main")
        )
        
        bot.send_message(
            call.message.chat.id,
            text,
            parse_mode='HTML',
            reply_markup=markup
        )
        
    except requests.exceptions.RequestException as e:
        bot.send_message(
            call.message.chat.id,
            f"‚ùå <b>–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã</b>\n\n"
            f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏.\n"
            f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–æ—Å—Ç—É–ø–Ω–∞: {url}",
            parse_mode='HTML'
        )
    except Exception as e:
        bot.send_message(
            call.message.chat.id,
            f"‚ùå <b>–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞</b>\n\n"
            f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}",
            parse_mode='HTML'
        )


print("‚úÖ handlers/website/article_analyzer.py –∑–∞–≥—Ä—É–∂–µ–Ω")
