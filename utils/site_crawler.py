# -*- coding: utf-8 -*-
"""
–ö—Ä–∞—É–ª–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–±–æ—Ä–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ —Å–∞–π—Ç–∞
"""
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import logging

logger = logging.getLogger(__name__)


def crawl_website(base_url, max_pages=50, timeout=30):
    """
    –ö—Ä–∞—É–ª–∏—Ç —Å–∞–π—Ç –∏ —Å–æ–±–∏—Ä–∞–µ—Ç –≤–∞–∂–Ω—ã–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
    
    Args:
        base_url: –ë–∞–∑–æ–≤—ã–π URL —Å–∞–π—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, https://ecosteni.ru)
        max_pages: –ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ö–æ–¥–∞
        timeout: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (—Å–µ–∫—É–Ω–¥—ã)
        
    Returns:
        dict: {
            'success': bool,
            'links': list of dict [{'url': str, 'title': str, 'priority': int}],
            'error': str
        }
    """
    try:
        start_time = time.time()
        base_domain = urlparse(base_url).netloc
        
        visited = set()
        to_visit = [base_url]
        important_links = []
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –ø—É—Ç–∏ (–≤–∞–∂–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã)
        priority_keywords = [
            '—É—Å–ª—É–≥', '—Ç–æ–≤–∞—Ä', '–ø—Ä–æ–¥—É–∫—Ç', '–∫–∞—Ç–µ–≥–æ—Ä', '–æ-–∫–æ–º–ø–∞–Ω–∏', 'about',
            '–∫–æ–Ω—Ç–∞–∫—Ç', 'contact', '—Ü–µ–Ω', 'price', '–ø–æ—Ä—Ç—Ñ–æ–ª', 'portfolio',
            '–ø—Ä–æ–µ–∫—Ç', 'work', '–æ—Ç–∑—ã–≤', 'review'
        ]
        
        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–µ –ø—É—Ç–∏
        ignore_patterns = [
            'wp-admin', 'wp-login', 'wp-content', 'wp-includes',
            'admin', 'login', 'register', 'cart', 'checkout',
            'search', 'feed', 'rss', 'sitemap.xml', 'robots.txt',
            '.jpg', '.png', '.gif', '.pdf', '.zip', '.css', '.js'
        ]
        
        print(f"üï∑ –ù–∞—á–∏–Ω–∞—é –∫—Ä–∞—É–ª–∏–Ω–≥: {base_url}")
        print(f"üìä –ú–∞–∫—Å. —Å—Ç—Ä–∞–Ω–∏—Ü: {max_pages}, —Ç–∞–π–º–∞—É—Ç: {timeout}—Å")
        
        while to_visit and len(visited) < max_pages:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∞–π–º–∞—É—Ç–∞
            if time.time() - start_time > timeout:
                print(f"‚è± –î–æ—Å—Ç–∏–≥–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç {timeout}—Å")
                break
            
            current_url = to_visit.pop(0)
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–µ
            if current_url in visited:
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–µ –ø—É—Ç–∏
            if any(pattern in current_url.lower() for pattern in ignore_patterns):
                continue
            
            visited.add(current_url)
            
            try:
                # –ó–∞–ø—Ä–æ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                response = requests.get(
                    current_url,
                    headers={
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    },
                    timeout=10,
                    allow_redirects=True
                )
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
                if response.status_code != 200:
                    print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ {current_url}: —Å—Ç–∞—Ç—É—Å {response.status_code}")
                    continue
                
                # –ü–∞—Ä—Å–∏–Ω–≥ HTML
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # –ü–æ–ª—É—á–∞–µ–º title —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                title_tag = soup.find('title')
                page_title = title_tag.get_text().strip() if title_tag else current_url
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                priority = 1  # –û–±—ã—á–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                url_lower = current_url.lower()
                
                # –ü–æ–≤—ã—à–µ–Ω–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤
                for keyword in priority_keywords:
                    if keyword in url_lower or keyword in page_title.lower():
                        priority = 2
                        break
                
                # –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                if current_url == base_url or current_url == base_url + '/':
                    priority = 3
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É
                if current_url != base_url:  # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º –≥–ª–∞–≤–Ω—É—é
                    important_links.append({
                        'url': current_url,
                        'title': page_title[:100],  # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                        'priority': priority
                    })
                
                print(f"‚úÖ [{len(visited)}/{max_pages}] {current_url[:60]}... (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {priority})")
                
                # –ò—â–µ–º –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                for link in soup.find_all('a', href=True):
                    href = link.get('href')
                    
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
                    absolute_url = urljoin(current_url, href)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Ç–æ—Ç –∂–µ –¥–æ–º–µ–Ω
                    if urlparse(absolute_url).netloc == base_domain:
                        # –£–±–∏—Ä–∞–µ–º —è–∫–æ—Ä—è –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                        clean_url = absolute_url.split('#')[0].split('?')[0]
                        
                        if clean_url not in visited and clean_url not in to_visit:
                            to_visit.append(clean_url)
                
                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(0.5)
                
            except requests.exceptions.RequestException as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {current_url}: {e}")
                continue
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {current_url}: {e}")
                continue
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        important_links.sort(key=lambda x: x['priority'], reverse=True)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ —Ç–æ–ø-30
        important_links = important_links[:30]
        
        print(f"\n‚úÖ –ö—Ä–∞—É–ª–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"üìä –ü–æ—Å–µ—â–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(visited)}")
        print(f"üîó –°–æ–±—Ä–∞–Ω–æ –≤–∞–∂–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(important_links)}")
        
        return {
            'success': True,
            'links': important_links,
            'total_visited': len(visited)
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫—Ä–∞—É–ª–∏–Ω–≥–∞: {e}")
        return {
            'success': False,
            'error': str(e),
            'links': []
        }


print("‚úÖ utils/site_crawler.py –∑–∞–≥—Ä—É–∂–µ–Ω")
